{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158879c4-18db-4d81-89eb-670e2676816e",
   "metadata": {},
   "source": [
    "### 문자열 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52f40f2-9e2a-44e1-8496-689d590d9c2d",
   "metadata": {},
   "source": [
    "#### 토큰\n",
    "\n",
    "- 문법적으로 더 이상 나눌 수 없는 언어 요소.\n",
    "- 영어의 경우 각 단어로 나누면 되지만 한글의 경우 복잡한 처리 과정을 거쳐야 하기 때문에 별도의 라이브러리를 적용해야 한다. (konlpy, mecab 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e57b5a-e48a-443a-8307-3d8d5d168b58",
   "metadata": {},
   "source": [
    "#### 영어 문장에 대한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202ad5fc-e2a7-43cb-b448-e366fa17a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 참조\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0585be39-f8e3-46c6-8745-b749535c4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 학습 데이터\n",
    "\n",
    "train_text = ['You are the Best', 'You are the Nice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527342d6-f758-46ec-bea8-af94581710f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 객체 생성\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 10, oov_token = '<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6272840d-5b69-4854-915f-0da96d5b21d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'you': 2, 'are': 3, 'the': 4, 'best': 5, 'nice': 6}\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 삭습\n",
    "\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d76b191-fb4f-4571-9efe-02f3cf127be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 4, 6]]\n"
     ]
    }
   ],
   "source": [
    "# 학습 결과 적용하기\n",
    "\n",
    "train_text = ['We are the Best', 'We are the Nice']\n",
    "sequences = tokenizer.texts_to_sequences(train_text)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d2419-48f5-426b-b0a8-514f71d32907",
   "metadata": {},
   "source": [
    "#### 한글 문장에 대한 토큰 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28897e4-73c1-4d94-916d-e44e5980d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecab 토큰 분석 엔진 설치 후 패키지 참조\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9390c1-d1dd-425c-b65b-5ef54862a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 학습 데이터 \n",
    "\n",
    "poem = \"\"\"\n",
    "흘러내린 머리카락이 흐린 호박빛 아래 빛난다.\n",
    "난 유영한다. 차분하게 과거에 살면서 현재의 공기를 마신다.\n",
    "가로등이 깜빡인다.\n",
    "나도 깜빡여준다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83515a14-4da0-4706-addf-dc3a8fa017f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어(분석에서 제외할 불필요한 단어) 목록 정의\n",
    "\n",
    "stopwords = ['난', '나']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f35fcd9d-45c0-455a-8621-5ef4e53dd270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['머리카락', '호박', '빛', '아래', '난', '유영', '과거', '현재', '공기', '가로등', '나']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석을 통해 명사만 추출\n",
    "\n",
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "nouns = mecab.nouns(poem)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a5d249-a982-4ec1-8a41-ba14bb3967f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['머리카락', '호박', '빛', '아래', '유영', '과거', '현재', '공기', '가로등']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사에서 불용어를 제외한 새로운 리스트 만들기\n",
    "\n",
    "train_text = []\n",
    "\n",
    "for i in nouns:\n",
    "    if i not in stopwords:\n",
    "        train_text.append(i)\n",
    "        \n",
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c19ee95-fe1d-4926-9c6c-bbf55ea82ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, '머리카락': 2, '호박': 3, '빛': 4, '아래': 5, '유영': 6, '과거': 7, '현재': 8, '공기': 9, '가로등': 10}\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 수행\n",
    "\n",
    "tokenizer = Tokenizer(num_words = len(nouns), oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a04a73-e367-411d-a349-d7e3898911f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
